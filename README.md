# LLMS Prompts (Q&As and More)


ChatGPT (4o mini)

Code

_Vanilla Python with/without NumPy_

- [Q: generate a perceptron in plain vanilla python code with numpy](perceptron/perceptron/)
   - note - generates a single-layer perceptron training on logicial xor, see [Q: explain if you can train with success a single-layer perceptron on logical xor](theory/non-linear/)
- [Q: generate a perceptron in plain vanilla python code with numpy and train on logical or](perceptron/perceptron-ii/)
  - [Q: try the code without numpy](perceptron/perceptron-iib/)

<!-- multi-layer perceptrons (mlp) -->

- [Q: generate a multi-layer perceptron in plain vanilla python code with numpy and train on logical xor](perceptron/perceptron-v2/)
  - [Q: try the code without numpy](perceptron/perceptron-v2b/)



_With PyTorch_

- [Q: generate a perceptron in python code with pytorch](perceptron-pytorch/perceptron-pytorch/)
   - note - generates a single-layer perceptron training on logicial xor, see [Q: explain if you can train with success a single-layer perceptron on logical xor](theory/non-linear/)
- [Q: generate a perceptron in python code with pytorch and train on logical or](perceptron-pytorch/perceptron-pytorch-ii/)


<!-- multi-layer perceptrons (mlp) -->

- [Q: generate a multi-layer perceptron in python code with pytorch and train on logical xor](perceptron-pytorch/perceptron-pytorch-v2)
  - [Q: change the neural network to use 3 hidden neurons and 2 outputs](perceptron-pytorch/perceptron-pytorch-v2b/)



Theory

- [Q: what is a single-layer perceptron?](theory/perceptron/)
- [Q: explain if you can train with success a single-layer perceptron on logical xor](theory/non-linear/)
- [Q: what is the best multi-layer perceptron configuration to train with success a perceptron on logical xor](theory/xor-model/)
  - note - includes running sample code in python with pytorch!
- [Q: what's the difference between gpt and bert neural networks?](theory/gpt-vs-bert/)
- [Q: can you explain the word2vec neural network model?](theory/word2vec/)
- [Q: what are some options for positional encoding of word embeddings?](theory/pos-encoding/)
- [Q: what are the pro and cons of byte pair encoding tokenizers compared to alternatives?](theory/bpe-vs-others/)
- [Q: can you explain transformers with attention mechanisms?](theory/transformers/)
- [Q: what differences are there in the neural network layers of gpt 2 and llama 3?](theory/gpt-vs-llama/)
- [Q: what are the memory requirements for gpt-2?](theory/gpt-memory/)
- [Q: how is the gpt-3 neural network architecture different from gpt-2?](theory/gpt2-vs-gpt3/)
- [Q: can large language model think?](theory/think/)



History

- [Q: timeline of a.i. research](history/timeline-ai/)
- [Q: timeline of a.i. research in neural networks](history/timeline-ai-nn/)
- [Q: timeline of a.i. research in neural networks with transformers and attention](history/timeline-ai-nn-transformers/)
- [Q: timeline of decoder-only transformer neural network models](history/timeline-ai-nn-transformers-decode/)





> that's all for today. thank you.
>
>> You're very welcome! I'm glad I could help.
>> If you have more questions in the future,
>> don't hesitate to reach out. Have a great day!

