# llms.sandbox

(Code) Sandbox



Sample Prompts


ChatGPT (4o mini)

Code

_Vanilla Python with/without NumPy_

- [Q: generate a perceptron in plain vanilla python code with numpy](perceptron/perceptron/)
   - note - generates a single-layer perceptron training on logicial xor, see [Q: explain if you can train with success a single-layer perceptron on logical xor](theory/non-linear/)
- [Q: generate a perceptron in plain vanilla python code with numpy and train on logical or](perceptron/perceptron-ii/)
  - [Q: try the code without numpy](perceptron/perceptron-iib/)

<!-- multi-layer perceptrons (mlp) -->

- [Q: generate a multi-layer perceptron in plain vanilla python code with numpy and train on logical xor](perceptron/perceptron-v2/)
  - [Q: try the code without numpy](perceptron/perceptron-v2b/)



_With PyTorch_

- [Q: generate a perceptron in python code with pytorch](perceptron-pytorch/perceptron-pytorch/)
   - note - generates a single-layer perceptron training on logicial xor, see [Q: explain if you can train with success a single-layer perceptron on logical xor](theory/non-linear/)
- [Q: generate a perceptron in python code with pytorch and train on logical or](perceptron-pytorch/perceptron-pytorch-ii/)


<!-- multi-layer perceptrons (mlp) -->

- [Q: generate a multi-layer perceptron in python code with pytorch and train on logical xor](perceptron-pytorch/perceptron-pytorch-v2)
  - [Q: change the neural network to use 3 hidden neurons and 2 outputs](perceptron-pytorch/perceptron-pytorch-v2b/)



Theory

- [Q: explain if you can train with success a single-layer perceptron on logical xor](theory/non-linear/)
- [Q: what is the best multi-layer perceptron configuration to train with success a perceptron on logical xor](theory/xor-model/)
  - note - includes running sample code in python with pytorch!
- [Q: what's the difference between gpt and bert neural networks?](theory/gpt-vs-bert/)



History

- [Q: timeline of a.i. research](history/timeline-ai/)
- [Q: timeline of a.i. research in neural networks](history/timeline-ai-nn/)
- [Q: timeline of a.i. research in neural networks with transformers and attention](history/timeline-ai-nn-transformers/)




