# llms.sandbox

(Code) Sandbox



Sample Prompts


ChatGPT (4o mini)

Code

_Vanilla Python with/without NumPy_

- [Q: generate a perceptron in plain vanilla python code with numpy](perceptron/)
   - note - generates a single-layer perceptron training on logicial xor, see [Q: explain if you can train with success a single-layer perceptron on logical xor](non-linear/)
- [Q: generate a perceptron in plain vanilla python code with numpy and train on logical or](perceptron-ii/)
  - [Q: try the code without numpy](perceptron-iib/)

<!-- multi-layer perceptrons (mlp) -->

- [Q: generate a multi-layer perceptron in plain vanilla python code with numpy and train on logical xor](percpetron-v2/)
  - [Q: try the code without numpy](percpetron-v2b/)



_With PyTorch_

- [Q: generate a perceptron in python code with pytorch](perceptron-pytorch/)
   - note - generates a single-layer perceptron training on logicial xor, see [Q: explain if you can train with success a single-layer perceptron on logical xor](non-linear/)
- [Q: generate a perceptron in python code with pytorch and train on logical or](perceptron-pytorch-ii/)


<!-- multi-layer perceptrons (mlp) -->

- [Q: generate a multi-layer perceptron in python code with pytorch and train on logical xor](perceptron-pytorch-v2)
  - [Q: change the neural network to use 3 hidden neurons and 2 outputs](perceptron-pytorch-v2b/)



Theory

- [Q: explain if you can train with success a single-layer perceptron on logical xor](non-linear/)
- [Q: what is the best multi-layer perceptron configuration to train with success a perceptron on logical xor](xor-model/)
  - note - includes running sample code in python with pytorch!
- [Q: what's the difference between gpt and bert neural networks?](gpt-vs-bert/)



